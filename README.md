# End-to-end guide: Summarizing client data with Ollama and .NET (Windows 11 + Visual Studio 2022)

This guide walks you through:
- Installing and running Ollama on Windows 11
- Integrating Ollama with a .NET 8 Web API in Visual Studio 2022
- Pulling data from SQL Server and sending it to an LLM
- Prompting the LLM to produce a structured, reliable summary
- Exposing a REST API endpoint your apps can call

The sample code below implements:
- GET /api/summaries/{clientId} to fetch a client's data from SQL Server and return a JSON summary generated by Ollama
- A clean separation of data access, prompt building, and LLM calling

--------------------------------------------------------------------------------

## 1) Install and run Ollama on Windows 11

1. Download and install
   - Go to https://ollama.com/download
   - Download the Windows installer and run it (requires Windows 11; CPU with AVX/AVX2 recommended)

2. Start the Ollama service
   - After install, open a new Terminal/PowerShell window
   - Pull a model (first run will download it):
     - `ollama pull llama3.1:8b`
       - Alternatives: `llama3.1:70b` (bigger, slower/stronger), `mistral:7b`, `qwen2:7b`, etc.
   - Quick test:
     - `ollama run llama3.1:8b "Say hello in one sentence."`
   - Ensure the REST API is up (default: http://localhost:11434):
     - PowerShell: `Invoke-WebRequest http://localhost:11434/api/tags`
     - You should get a JSON list of local models

3. Optional: set model and server variables
   - Default base URL: `http://localhost:11434`
   - You can configure your .NET app to point to it via appsettings.json (see below)

4. Notes
   - First generation after a fresh model pull can be slower (warm-up)
   - GPU acceleration on Windows depends on your setup and drivers; CPU-only works fine to start
   - Keep the Ollama service running while developing/testing

--------------------------------------------------------------------------------

## 2) Create the .NET Web API in Visual Studio 2022

We’ll create an ASP.NET Core 8 Web API that:
- Connects to SQL Server (via Dapper for simplicity)
- Calls Ollama’s REST API (via HttpClient)
- Returns a structured JSON summary

### Steps
1. In Visual Studio 2022: File > New > Project > ASP.NET Core Web API
   - Target Framework: .NET 8.0
   - Project name (example): `OllamaSummary.Api`

2. Add NuGet packages:
   - Dapper
   - Microsoft.Data.SqlClient
   - (Optional) System.Text.Json (already included in .NET 8)
   - You can do this via Package Manager Console:
     - `Install-Package Dapper`
     - `Install-Package Microsoft.Data.SqlClient`

3. Add the following files from this guide into your project:
   - appsettings.json (configure your connection string and Ollama base URL/model)
   - Program.cs
   - Controllers/SummariesController.cs
   - Services/OllamaClient.cs
   - Data/ClientDataRepository.cs
   - Models/*.cs (DTOs)

4. Update appsettings.json
   - Set your SQL Server connection string
   - Adjust the Ollama model if you prefer a different one

5. Build and run the API
   - The endpoint will listen (by default) on https://localhost:PORT and http://localhost:PORT
   - Test once you have some data in SQL: `GET /api/summaries/{clientId}`

--------------------------------------------------------------------------------

## 3) How the data flows

- Your application or caller invokes: `GET /api/summaries/{clientId}`
- The API:
  - Queries SQL Server for:
    - Wealth records (assets, liabilities, net worth over time)
    - Financial status (income, expenses, cashflow, risk profile, goals)
    - Advisor meeting notes (free text)
  - Builds a concise context prompt
  - Calls Ollama’s `/api/chat` with a system + user message
  - Requests a JSON-structured answer for reliability
  - Returns the JSON to the caller

This pattern can serve as your internal REST API to the LLM. Your UI (React, mobile, etc.) calls this API; the API calls Ollama locally.

--------------------------------------------------------------------------------

## 4) Prompting strategy

- Use a system prompt to set consistent behavior: role, tone, constraints, output schema
- Use a user prompt that supplies the client data in structured sections
- Ask for JSON output with a schema that you control (and consider using `"format": "json"` when supported by the model)
- Keep the overall input within the model’s context window (e.g., 8K tokens). If you have long meeting notes, consider:
  - Summarizing notes per meeting first (map-reduce)
  - Chunking notes and extracting key points
  - Storing embeddings and retrieving only relevant chunks (RAG). Ollama supports `/api/embeddings`—you can roll your own vector store.

--------------------------------------------------------------------------------

## 5) Sending data to the LLM via REST

Ollama provides a local REST API:
- Chat endpoint (recommended): POST http://localhost:11434/api/chat
- Typical body:
  ```json
  {
    "model": "llama3.1:8b",
    "messages": [
      {"role": "system", "content": "…instructions…"},
      {"role": "user", "content": "…your data and request…"}
    ],
    "stream": false,
    "options": {
      "temperature": 0.2,
      "num_ctx": 8192
    },
    "format": "json"
  }
  ```
- Response returns the assistant’s message. When `stream: false`, you’ll get the full content in one payload.

In the sample code:
- `OllamaClient` wraps calls to `/api/chat`
- You pass the constructed prompt and get back the summary JSON

--------------------------------------------------------------------------------

## 6) SQL Server considerations

- Only fetch data needed for the summary
- Normalize and pre-aggregate on the server side where possible (reduce token usage)
- Be careful with PII—keep Ollama local or implement data masking if needed
- Index meeting notes (e.g., full-text search) if you need topic retrieval
- For high volume, consider caching per-client summaries and invalidating on data changes

--------------------------------------------------------------------------------

## 7) Testing

- Ensure Ollama is running and the model is pulled
- Test the API:
  - Example: `GET https://localhost:5001/api/summaries/12345`
  - You should receive a JSON object with fields like `client_overview`, `wealth_summary`, etc.
- If you want to see the raw prompt being sent, temporarily log it (redact PII in logs for production)

--------------------------------------------------------------------------------

## 8) Production tips

- Timeouts: Large prompts can take longer; set reasonable HttpClient timeouts
- Retries: Handle transient failures gracefully
- Observability: Log request IDs, durations, token counts (approximate), and model used
- Guardrails: Validate JSON output; if parsing fails, retry with a stricter instruction
- Security: Keep Ollama bound to localhost or secure the network if remote
- Scale: Consider running Ollama on a workstation or server with more RAM/CPU/GPU for larger models

--------------------------------------------------------------------------------

## 9) Example data model and prompt schema

We’ll ask the model to output JSON like:
```json
{
  "client_overview": "...",
  "wealth_summary": {
    "net_worth_now": "...",
    "trend": "...",
    "asset_allocation": "...",
    "liabilities_summary": "..."
  },
  "financial_status_summary": {
    "income": "...",
    "expenses": "...",
    "cashflow": "...",
    "risk_profile": "...",
    "goals": "..."
  },
  "meeting_notes_summary": {
    "key_points": ["..."],
    "decisions": ["..."],
    "follow_ups": ["..."]
  },
  "key_risks": ["..."],
  "recommended_actions": ["..."],
  "data_freshness": "As of YYYY-MM-DD",
  "caveats": ["..."]
}
```

We enforce this via the system message and by passing `"format": "json"` to Ollama (when supported by the selected model). If the model struggles with strict JSON, remove `"format"` and rely on prompting, then robustly parse the result (or ask the model to “respond only with valid JSON, no commentary”).

--------------------------------------------------------------------------------

## 10) What’s included in the sample code

- A minimal ASP.NET Core Web API
- Dapper-based repository to query SQL Server
- Ollama client to call `/api/chat`
- A controller endpoint that ties it together
- Config via appsettings.json

Review and adapt queries to your schema.

--------------------------------------------------------------------------------

## 11) Next steps and enhancements

- Add authentication/authorization (e.g., Azure AD, OAuth)
- Add streaming support (show partial model responses to the user)
- Introduce a vector store for retrieval-augmented generation (RAG)
- Batch or schedule summaries (e.g., nightly report generation)
- Store summaries back in SQL for audit/tracking

--------------------------------------------------------------------------------

```